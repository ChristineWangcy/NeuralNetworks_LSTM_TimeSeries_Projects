{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Time Series Forecasting daily temperatures(LSTM).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "import time #helper libraries\n",
        "from tensorflow.keras import regularizers, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "fWkReWdVSelD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-TiyUuxf-fe"
      },
      "source": [
        "class ForecastingToolkit(object):\n",
        "    def __init__(self, df = None, model = None):\n",
        "        \"\"\"\n",
        "        Variables that we passed into our functions should now be defined \n",
        "        as class attributes, i.e. class variables. \n",
        "        \"\"\"\n",
        "        # here are a few to get you started \n",
        "        \n",
        "        # store data here\n",
        "        self.df = df\n",
        "        self.Y_train_predict = None\n",
        "        self.Y_test_predict = None\n",
        "        \n",
        "        # store your forecasting model here\n",
        "        self.model = model\n",
        "        \n",
        "        # store feature scalers here\n",
        "        self.scaler_data = None\n",
        "        self.scaler_dict = None\n",
        "\n",
        "        # store the training results of your model here\n",
        "        self.history = None\n",
        "    \n",
        "    def load_transform_data(self): \n",
        "        # load the energy data\n",
        "        energy_filepath = '/content/power_usage_2016_to_2020.csv'\n",
        "        weather_filepath = '/content/weather_2016_2020_daily.csv'\n",
        "        df_energy = pd.read_csv(energy_filepath, parse_dates=['StartDate'], index_col='StartDate')\n",
        "        df_energy_day = df_energy.resample('D').sum()\n",
        "\n",
        "        # # load the weather data\n",
        "        df_weather = pd.read_csv(weather_filepath, parse_dates=['Date'], index_col='Date')\n",
        "\n",
        "        # merge the two dataframes\n",
        "        df = pd.merge(df_energy_day, df_weather, left_index=True, right_index=True)\n",
        "\n",
        "        # cleanup\n",
        "        df = df.drop(columns=['day_of_week_x', 'Day'])\n",
        "\n",
        "        # rename column\n",
        "        df = df.rename(columns={'Value (kWh)': 'kwh'})\n",
        "\n",
        "        # truncate the dates (trim early and late dates)\n",
        "        # to drop data that's only weekly\n",
        "        self.df = df.loc[\"2016-06-01\": '2020-01-01']\n",
        "\n",
        "    def scale_data(self):\n",
        "        scaler_dict = {}\n",
        "        scaled_data = {}\n",
        "    \n",
        "        for col in self.df.columns:\n",
        "        \n",
        "          # instantiate the scaler class \n",
        "          scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "          \n",
        "          # reshape to avoid shape errors\n",
        "          feat = self.df[col].values.reshape(-1, 1)\n",
        "          \n",
        "          # scale data\n",
        "          col_scaled = scaler.fit_transform(feat)\n",
        "          \n",
        "          # dictionary with scaled data for each column\n",
        "          scaled_data[col] = col_scaled.flatten()\n",
        "          scaler_dict[col] = scaler\n",
        "        \n",
        "        # move scaled data from dict to dataframe\n",
        "        input_cols = ['kwh', 'Temp_avg', 'Dew_avg', 'Press_avg']\n",
        "        self.scaled_data = pd.DataFrame.from_dict(scaled_data)[input_cols]\n",
        "        self.scaler_dict = scaler_dict\n",
        "    \n",
        "    def invert_scaling(self, data, output_feat_name):\n",
        "        self.Y_train = self.scaler_dict[output_feat_name].inverse_transform(self.Y_train)\n",
        "        self.Y_test = self.scaler_dict[output_feat_name].inverse_transform(self.Y_test)\n",
        "        self.Y_train_predict = self.scaler_dict[output_feat_name].inverse_transform(self.Y_train_predict)\n",
        "        self.Y_test_predict = self.scaler_dict[output_feat_name].inverse_transform(self.Y_test_predict)\n",
        "    \n",
        "    def create_dataset(self, data, look_back=None, look_ahead=None, predict_only_last=None):\n",
        "        X_data, Y_data = [], []\n",
        "        n_samples = len(data)\n",
        "        window_length = look_back + look_ahead \n",
        "        n_sequences = n_samples - window_length + 1\n",
        "        print(f'Creating {n_sequences} X, Y samples')\n",
        "        if data.shape[1] > 1:\n",
        "            y_data = data[:, 0]\n",
        "        else:\n",
        "            y_data = data\n",
        "\n",
        "        for i in range(n_sequences):\n",
        "            x = data[i : i+look_back]\n",
        "            y = y_data[i+look_back : i + look_back + look_ahead] \n",
        "\n",
        "            if(predict_only_last):\n",
        "              y = y[-1] \n",
        "            X_data.append(x)\n",
        "            Y_data.append(y)\n",
        "        \n",
        "        return np.array(X_data), np.array(Y_data)\n",
        "    \n",
        "    def create_train_test_split(self):\n",
        "        \"\"\"\n",
        "        Creates a train test split for sequential data used for time series forecasting. \n",
        "        \"\"\"\n",
        "        look_back = 28\n",
        "        look_ahead = 7\n",
        "        train_size = 0.70\n",
        "        predict_only_last = False\n",
        "        df = self.scaled_data\n",
        "        # calculate the number of training samples \n",
        "        n_samples = df.shape[0]\n",
        "        train_size = int(n_samples * train_size)\n",
        "\n",
        "        # consecutive samples indexed from zero up to but not including train_size are the training samples \n",
        "        train = df.iloc[:train_size].values\n",
        "        \n",
        "        # consecutive samples indexed from train_size up to the end are the test samples \n",
        "        test = df.iloc[train_size:].values\n",
        "\n",
        "        # create input and output splits \n",
        "        X_train, Y_train = self.create_dataset(train, look_back=look_back, look_ahead=look_ahead, predict_only_last=predict_only_last)\n",
        "        X_test, Y_test = self.create_dataset(test, look_back=look_back, look_ahead=look_ahead, predict_only_last=predict_only_last)\n",
        "        \n",
        "        return X_train, Y_train, X_test, Y_test\n",
        "      \n",
        "    def build_model(self):\n",
        "        # this model architecture is arbitrary - you can experiment with different architectures to see how it affects the score (i.e. gridsearch)\n",
        "\n",
        "        # set hyperparameter values\n",
        "        epochs = 25\n",
        "        batch_size = 32\n",
        "        dropout_prob = 0.5\n",
        "        look_ahead = 7\n",
        "\n",
        "        # input shape is look_back rows by n_feats columns, for each element of the batch\n",
        "        input_shape = (28, 4)\n",
        "\n",
        "        # set learning rate and optimizer\n",
        "        opt = optimizers.Nadam(learning_rate=0.01)\n",
        "\n",
        "        # Create and train model here\n",
        "        model = Sequential()\n",
        "\n",
        "        # single LSTM layer\n",
        "        model.add(LSTM(256, input_shape=input_shape, activation='tanh', return_sequences=False))\n",
        "\n",
        "        # add dropout regularization\n",
        "        model.add(Dropout(dropout_prob))\n",
        "\n",
        "        # output layer WE'RE DOING REGRESSION, we want to predict continuous data\n",
        "        model.add(Dense(look_ahead, activation='relu'))\n",
        "\n",
        "        # compile model\n",
        "        model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
        "        model.summary()\n",
        "\n",
        "        self.model = model\n",
        "    \n",
        "    def fit_model(self):\n",
        "        early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1.e-6)\n",
        "        X_train, Y_train, X_test, Y_test = self.create_train_test_split()\n",
        "        print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
        "        history = self.model.fit(X_train, \n",
        "                            Y_train,\n",
        "                            epochs=100,\n",
        "                            batch_size=32,\n",
        "                            verbose=1,\n",
        "                            validation_data=(X_test, Y_test),\n",
        "                            callbacks = [early_stopping])\n",
        "        self.history = history\n",
        "    \n",
        "    def predict(self):\n",
        "        # make predictions on train and test inputs \n",
        "        X_train = self.X_train\n",
        "        X_test = self.X_test\n",
        "        Y_train_predict = self.model.predict(X_train)\n",
        "        Y_test_predict = self.model.predict(X_test)\n",
        "    \n",
        "    def plot_model_loss_metrics(self):\n",
        "        \"\"\"\n",
        "        Use the model history callback to plot the train and test losses vs epochs as well as metrics vs. epochs \n",
        "        \"\"\"\n",
        "        history = self.history\n",
        "        # plot training and test loss scores \n",
        "        test_loss = history.history[\"val_loss\"]\n",
        "        train_loss = history.history[\"loss\"]\n",
        "        \n",
        "        test_mae = history.history[\"val_mean_absolute_error\"]\n",
        "        train_mae = history.history[\"mean_absolute_error\"]\n",
        "        \n",
        "        n_epochs = len(test_loss) + 1\n",
        "        epochs = np.arange(1,  n_epochs)\n",
        "        y_ticks = np.arange(0, 1, 11)\n",
        "\n",
        "        plt.figure(figsize=(20,5))\n",
        "        plt.title(\"Loss vs. Number of Epochs\")\n",
        "        plt.plot(epochs[1:], test_loss[1:], label = \"Test Loss\")\n",
        "        plt.plot(epochs[1:], train_loss[1:], label = \"Train Loss\")\n",
        "        plt.xlim(1,20)\n",
        "        plt.xticks(epochs[1:])\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show() \n",
        "        \n",
        "        plt.figure(figsize=(20,5))\n",
        "        plt.title(\"mean_absolute_error vs. Number of Epochs\")\n",
        "        plt.plot(epochs[1:], test_mae[1:], label = \"Test MAE\")\n",
        "        plt.plot(epochs[1:], train_mae[1:], label = \"Train MAE\")\n",
        "        plt.xlim(1,20)\n",
        "        plt.xticks(epochs[1:])\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show() \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1JmDnpsf-fg"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNwtm_gif-fh"
      },
      "source": [
        "# once you've completed your class, you'll be able to perform a many operations with just a few lines of code!\n",
        "tstk = ForecastingToolkit()\n",
        "tstk.load_transform_data()\n",
        "tstk.scale_data()\n",
        "tstk.build_model()\n",
        "tstk.fit_model()\n",
        "tstk.plot_model_loss_metrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use LSTM to predict future 7 days minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia."
      ],
      "metadata": {
        "id": "OW4KuWaWwmQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "look_ahead = 7"
      ],
      "metadata": {
        "id": "yZVJXOsznELc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_transform_data(): \n",
        "    # load the energy data\n",
        "    filepath = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'], index_col='Date')\n",
        "    return df\n",
        "df = load_transform_data()"
      ],
      "metadata": {
        "id": "1-vXVEpHagAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_data(df):\n",
        "    scaler_dict = {}\n",
        "    scaled_data = {}\n",
        "\n",
        "    for col in df.columns:\n",
        "    \n",
        "      # instantiate the scaler class \n",
        "      scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "      \n",
        "      # reshape to avoid shape errors\n",
        "      feat = df[col].values.reshape(-1, 1)\n",
        "      \n",
        "      # scale data\n",
        "      col_scaled = scaler.fit_transform(feat)\n",
        "      \n",
        "      # dictionary with scaled data for each column\n",
        "      scaled_data[col] = col_scaled.flatten()\n",
        "    \n",
        "    # move scaled data from dict to dataframe\n",
        "    return pd.DataFrame.from_dict(scaled_data)\n",
        "df = scale_data(df)"
      ],
      "metadata": {
        "id": "HJMjWJZnalgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(data, look_back=None, look_ahead=None, predict_only_last=None):\n",
        "    X_data, Y_data = [], []\n",
        "    n_samples = len(data)\n",
        "    window_length = look_back + look_ahead \n",
        "    n_sequences = n_samples - window_length + 1\n",
        "    print(f'Creating {n_sequences} X, Y samples')\n",
        "    '''\n",
        "    if data.shape[1] > 1:\n",
        "        y_data = data[:, 0]\n",
        "    else:\n",
        "        y_data = data\n",
        "    '''\n",
        "    y_data = data[:, 0]\n",
        "    for i in range(n_sequences):\n",
        "        x = data[i : i+look_back]\n",
        "        y = y_data[i+look_back : i + look_back + look_ahead] \n",
        "\n",
        "        if(predict_only_last):\n",
        "          y = y[-1] \n",
        "        X_data.append(x)\n",
        "        Y_data.append(y)\n",
        "    \n",
        "    return np.array(X_data), np.array(Y_data)"
      ],
      "metadata": {
        "id": "s1tVUU9tbssg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_test(df):\n",
        "    \"\"\"\n",
        "    Creates a train test split for sequential data used for time series forecasting. \n",
        "    \"\"\"\n",
        "    look_back = 30\n",
        "    train_size = 0.70\n",
        "    predict_only_last = False\n",
        "    # calculate the number of training samples \n",
        "    n_samples = df.shape[0]\n",
        "    train_size = int(n_samples * train_size)\n",
        "\n",
        "    # consecutive samples indexed from zero up to but not including train_size are the training samples \n",
        "    train = df.iloc[:train_size].values\n",
        "    \n",
        "    # consecutive samples indexed from train_size up to the end are the test samples \n",
        "    test = df.iloc[train_size:].values\n",
        "    X_train, Y_train = create_dataset(train, look_back=look_back, look_ahead=look_ahead, predict_only_last=predict_only_last)\n",
        "    X_test, Y_test = create_dataset(test, look_back=look_back, look_ahead=look_ahead, predict_only_last=predict_only_last)\n",
        "        \n",
        "    return X_train, Y_train, X_test, Y_test"
      ],
      "metadata": {
        "id": "N5m4w_5qa3fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = create_train_test(df)"
      ],
      "metadata": {
        "id": "fk3SJkwmb-cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "id": "5UmWecNkhJgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(epochs = 25, batch_size = 32, dropout_prob = 0.5, look_ahead = 7):\n",
        "    # this model architecture is arbitrary - you can experiment with different architectures to see how it affects the score (i.e. gridsearch)\n",
        "\n",
        "    # set hyperparameter values\n",
        "    epochs = epochs\n",
        "    batch_size = batch_size\n",
        "    dropout_prob = dropout_prob\n",
        "    look_ahead = look_ahead\n",
        "\n",
        "    # input shape is look_back rows by n_feats columns, for each element of the batch\n",
        "    input_shape = (30, 1)\n",
        "\n",
        "    # set learning rate and optimizer\n",
        "    opt = optimizers.Nadam(learning_rate=0.01)\n",
        "\n",
        "    # Create and train model here\n",
        "    model = Sequential()\n",
        "\n",
        "    # single LSTM layer\n",
        "    model.add(LSTM(256, input_shape=input_shape, activation='tanh', return_sequences=False))\n",
        "\n",
        "    # add dropout regularization\n",
        "    model.add(Dropout(dropout_prob))\n",
        "\n",
        "    # output layer WE'RE DOING REGRESSION, we want to predict continuous data\n",
        "    model.add(Dense(look_ahead, activation='relu'))\n",
        "\n",
        "    # compile model\n",
        "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
        "    model.summary()\n",
        "    return model "
      ],
      "metadata": {
        "id": "Py0z63MIclpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "metadata": {
        "id": "Pxw_KvHwe0Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# fit model\n",
        "\n",
        "# Create and run Grid Search\n",
        "# build out our hyperparameter dictionary \n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1.e-6)\n",
        "hyper_parameters = { \n",
        "    \"epochs\": [10, 50, 100],\n",
        "    \"dropout_prob\": np.linspace(0.0, 0.6, num=3),\n",
        "    \"callbacks\": [early_stopping],\n",
        "    \"validation_data\": [(X_test, Y_test)],\n",
        "    \"batch_size\": [32]\n",
        "    } \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-3, \n",
        "                    verbose=1, \n",
        "                    cv=2)\n",
        "\n",
        "grid_result = grid.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "Pf1G_poIcvGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dir(grid_result)\n",
        "grid_result.best_params_"
      ],
      "metadata": {
        "id": "C5OFgMocqfKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_result.best_estimator_.build_fn(epochs = 10, batch_size = 32, dropout_prob = 0, look_ahead = 7)\n",
        "history = best_model.fit(X_train, Y_train, \n",
        "                    epochs=10, \n",
        "                    batch_size=32, \n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "VzSYIKeZlGhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "zK_0YpUTscIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_loss_metrics(history):\n",
        "    \"\"\"\n",
        "    Use the model history callback to plot the train and test losses vs epochs as well as metrics vs. epochs \n",
        "    \"\"\"\n",
        "    history = history\n",
        "    # plot training and test loss scores \n",
        "    test_loss = history.history[\"loss\"]\n",
        "    train_loss = history.history[\"loss\"]\n",
        "    \n",
        "    test_mae = history.history[\"val_mean_absolute_error\"]\n",
        "    train_mae = history.history[\"mean_absolute_error\"]\n",
        "    \n",
        "    n_epochs = len(test_loss) + 1\n",
        "    epochs = np.arange(1,  n_epochs)\n",
        "    y_ticks = np.arange(0, 1, 11)\n",
        "\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.title(\"Loss vs. Number of Epochs\")\n",
        "    plt.plot(epochs[1:], test_loss[1:], label = \"Test Loss\")\n",
        "    plt.plot(epochs[1:], train_loss[1:], label = \"Train Loss\")\n",
        "    plt.xlim(1,20)\n",
        "    plt.xticks(epochs[1:])\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show() \n",
        "    \n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.title(\"mean_absolute_error vs. Number of Epochs\")\n",
        "    plt.plot(epochs[1:], test_mae[1:], label = \"Test MAE\")\n",
        "    plt.plot(epochs[1:], train_mae[1:], label = \"Train MAE\")\n",
        "    plt.xlim(1,20)\n",
        "    plt.xticks(epochs[1:])\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show() "
      ],
      "metadata": {
        "id": "QJ_wIiz4k9hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_loss_metrics(history)"
      ],
      "metadata": {
        "id": "AYt97c8ulPng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train, X_test):\n",
        "    # make predictions on train and test inputs \n",
        "    Y_train_predict = best_model.predict(X_train)\n",
        "    Y_test_predict = best_model.predict(X_test)\n",
        "    \n",
        "    return Y_train_predict, Y_test_predict\n",
        "\n",
        "Y_train_predict, Y_test_predict = predict(X_train, X_test)"
      ],
      "metadata": {
        "id": "QvMhFyrBmSbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHFjtQ8QmFvm"
      },
      "source": [
        "## YOUR CODE HERE \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def invert_scaling(data):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    feat = df['Temp'].values.reshape(-1, 1)\n",
        "    scaler.fit_transform(feat)\n",
        "    return scaler.inverse_transform(data)\n",
        "Y_train = invert_scaling(Y_train)\n",
        "Y_test = invert_scaling(Y_test)\n",
        "Y_train_predict = invert_scaling(Y_train_predict)\n",
        "Y_test_predict = invert_scaling(Y_test_predict)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(Y_train, Y_train_predict, Y_test, Y_test_predict):\n",
        "    \n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.title(\"Training Set: True vs Predicted temp\")\n",
        "    plt.grid()\n",
        "    plt.plot(Y_train_predict[:,0], label = \"Predict\", c=\"r\")\n",
        "    plt.plot(Y_train[:,0], label= \"True\", c=\"c\")\n",
        "    plt.xlim((0,300))\n",
        "    plt.legend();\n",
        "    \n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.title(\"Test Set: True vs Predicted temp\")\n",
        "    plt.grid()\n",
        "    plt.plot(Y_test_predict[:,0], label = \"Predict\", c=\"r\")\n",
        "    plt.plot(Y_test[:,0], label= \"True\", c=\"c\")\n",
        "    plt.xlim((0,300))\n",
        "    plt.legend();\n",
        "\n",
        "plot_predictions(Y_train, Y_train_predict, Y_test, Y_test_predict) "
      ],
      "metadata": {
        "id": "qCgx31yomjvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Model against a Naive Baseline"
      ],
      "metadata": {
        "id": "NyjSCZA1msdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_scores\n",
        "print(f'Scores when forecasting with our LSTM model on the test data:')\n",
        "for day in range(look_ahead):\n",
        "  test_score_ = math.sqrt(mean_squared_error(Y_test[:,day], Y_test_predict[:,day],))\n",
        "  print(f'RMSE = {test_score_:.2f} temp for forecasting {day+1} day(s) in the future')\n",
        "test_score_all = math.sqrt(mean_squared_error(Y_test, Y_test_predict))\n",
        "print(f'Mean RMSE = {test_score_all:.2f} temp for forecasting all {look_ahead} look_ahead days in the future')\n",
        "\n",
        "# naive scores i.e. assuming that future kwh consumption will be the same as today for look_ahead days in the future\n",
        "print(f'\\nCompare with naive baseline scores:')\n",
        "for day in range(1, look_ahead+1):\n",
        "  y1 = [Y_test[i][0] for i in range(0,len(Y_test) - day)]\n",
        "  y2 = [Y_test[j][0] for j in range(day,len(Y_test))]\n",
        "  naive_score = math.sqrt(mean_squared_error(y1,y2))\n",
        "  print(f'RMSE = {naive_score:.2f} temp for forecasting {day} day(s) in the future') "
      ],
      "metadata": {
        "id": "7ET1GWOFmxww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}